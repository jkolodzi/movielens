---
title: "Movie Prediction System"
author: "Justin A. Kolodziej"
date:
output: pdf_document
---

# 1. Introduction

A key feature of a movie streaming service such as NetFlixÂ® is to recommend movies
to users that they will rate highly based on their own ratings and the ratings of 
other users. The goal of the project is to develop a machine learning algorithm
that will minimize root mean squared error (RMSE target <= 0.8649) of movie ratings on a scale of
0 to 5 stars, using the 10 million rating version of the MovieLens database. Multiple
methods are used to identify patterns in a representative subsample of the data, and
algorithms and tuning parameters selected that run in a reasonable time and give
reasonably accurate results. In the end a hybrid approach is selected and run on the
full dataset.

# 2. Methods/Analysis

```{r, download-data, echo=FALSE, results="hide",message=FALSE,warning=FALSE}
knitr::opts_chunk$set(message=FALSE, warning=FALSE)
##########################################################
# Create edx set, validation set (final hold-out test set)
##########################################################

# Note: this process could take a couple of minutes

if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")

library(tidyverse)
library(caret)
library(data.table)

# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")

# if using R 3.6 or earlier:
#movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(levels(movieId))[movieId],
#                                            title = as.character(title),
#                                            genres = as.character(genres))
# if using R 4.0 or later:
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(movieId),
                                            title = as.character(title),
                                            genres = as.character(genres))


movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(1)`
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>% 
      semi_join(edx, by = "movieId") %>%
      semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)
```
The MovieLens data set contains the following columns:
```{r, print-names,echo=FALSE}
head(edx)
```
The "userid", "movieid", "rating", and "title" columns appear straightforward, while 
the "timestamp" and "genres" columns could use additional processing. There might be 
patterns in the year/month/day of rating at least, as well as whether a movie is or is 
not a drama, action, crime, etc. movie, and it may simplify analysis over having 
separate levels of a factor for "Action" vs. "Action|Adventure". The lack of movies in
many genres, e.g. "Action|Adventure|Comedy|Horror|Crime|Western|Sci-Fi|Fantasy|IMAX"
means that adding these columns adds to the storage requirements.

The "edx" dataset portion is split into a "train" and "test" set with 80% of the data in the "train" and 20% in the "test" dataset for analysis.

```{r, create-wrangle-train-test, echo=FALSE, results="hide"}
###################################################################################################################################
# BEGIN transformation of edx data
###################################################################################################################################
memory.limit(32000) #unnecessary on large or non-Windows machines
if(!require(lubridate)) install.packages("lubridate", repos = "http://cran.us.r-project.org")
if(!require(rpart)) install.packages("rpart", repos = "http://cran.us.r-project.org")

library(lubridate)
library(rpart)

#standard RMSE function
RMSE <- function(y, y_hat){
    sqrt(mean((y - y_hat)^2))
}

set.seed(2, sample.kind="Rounding") # if using R 3.5 or earlier, use `set.seed(2)`

#Further carve out a training and test set (80% training, 20% test) for algorithm and data exploration
test_index <- createDataPartition(y = edx$rating, times = 1, p = 0.2, list = FALSE)
train <- edx[-test_index,]
temp <- edx[test_index,]
test <- temp %>% 
    semi_join(train, by = "movieId") %>%
    semi_join(train, by = "userId")

# Add rows removed from test set back into train set
removed <- anti_join(temp, test)
train <- rbind(train, removed)
rm(removed)

# Release year, and year, month, day, week rated may be relevant for predicting ratings.
# The genres column may not be convenient for analysis. Convert it into an essentially binary 1 or 0 for
# each listed genre in case that affects users' ratings of movies.
wrangle <- function(ratings) {
    pattern <- "\\((\\d{4})\\)$"
    ratings <- ratings %>% mutate(releaseyear = as.numeric(str_match(title, pattern)[,2]),
                           year = year(as_datetime(timestamp)),
                           month = month(as_datetime(timestamp)),
                           day = day(as_datetime(timestamp)),
                           week = as.numeric(round_date(as_datetime(timestamp),unit="week"))) %>%
#                          movieId = factor(movieId),
#                          userId = factor(userId),
#                          rating = factor(rating)) %>%
                           select(-title) %>%
                           select(-timestamp)

    Genres <- str_split(ratings$genres, "[|]", simplify = TRUE) # produces one row per rating
    Genres <- sort(unique(Genres[Genres != "" & Genres != "(no genres listed)"])) # gathers all unique genre names into a single array
    print(Genres)
    for(g in Genres) {
        x <- !is.na(str_extract(ratings$genres, g))
        x <- data.frame(dummy = ifelse(x, 1, 0))
        colnames(x) <- str_remove(g,"-")
        ratings <- cbind(ratings, x)
    }
    ratings <- ratings %>% select(-genres)
    return(ratings)
}
train <- wrangle(train)
test <- wrangle(test)
```
```{r graph-tree, echo=FALSE}
head(train)
head(test)
```
##2.1 Exploration with regression trees

Since the wrangled dataset now has a large number of binary predictors, a regression
tree would seem to be a natural choice to determine which predictors are more important
in predicting movie ratings. By using a large complexity parameter (cp = .0025) the
tree can be made simple enough to graph as a starting point.

```{r print-test-train, echo=FALSE}
#train() function is taking too much memory/time on this dataset. I wrote functions to tune by cp and minsplit manually.
rpart_predict <- function(cp, train, test) {
    library(rpart)
    RMSE <- function(y, y_hat){
        sqrt(mean((y - y_hat)^2))
    }
    fit <- rpart(rating ~ ., data=train, control=rpart.control(cp = cp, maxsurrogate=0))
    print(fit)
    return(RMSE(test$rating,predict(fit, newdata=test))) 
}

rpart_predict2 <- function(minsplit, train, test) {
    library(rpart)
    RMSE <- function(y, y_hat){
        sqrt(mean((y - y_hat)^2))
    }
    fit <- rpart(rating ~ ., data=train, control=rpart.control(minsplit = minsplit, cp=0, maxsurrogate=0))
    print(fit)
    return(RMSE(test$rating,predict(fit, newdata=test))) 
}

#rpart gives a Variable Importance list with how much each factor affects the response. For relatively
#large values of cp the tree is also small enough to plot.

fit <- rpart(rating ~ ., data=train, control=rpart.control(cp = .0025, maxsurrogate=0))
plot(fit, margin = 0.1)
text(fit, cex = 0.75)
print(fit$variable.importance)
```
Most of the splits are by genre, so at a gross level using genre to predict rating 
appears to be a reasonable idea.

The regression tree can be tuned by minimum node size in addition to complexity 
parameter. It seems preferable, in fact, to avoid situations where some nodes have only 
a few observations and others have tens of thousands. Once an optimal node size is 
found, the variable importance can be graphed to see what predictors are most useful.
```{r tune-rpart, echo=FALSE}
#rpart is fairly memory efficient so I can tune it in parallel to some degree.
library(parallel)
cl <- makeCluster(3)
tune3 <- c(100000,50000,20000,10000,5000,2000,1000,500,200)
rtree_tune3 <- parSapplyLB(cl, tune3, rpart_predict2, train, test)
plot(tune3, rtree_tune3, type="b")
cat("Best RMSE =", min(rtree_tune3), "\n")
stopCluster(cl)

#Best minsplit is 500, though the results are not great. Plot the variable importance.
fit <- rpart(rating ~ ., data=train, control=rpart.control(cp = 0,minsplit = 500, maxsurrogate=0))
barplot(fit$variable.importance, las=2)

```


The best prediction appears to be a minsplit of 500 and the best predictors at that 
level appear to be movieId, releaseyear, Drama, and week. However the minimum RMSE is 
>0.94 so other approaches will be needed.

## 2.2 Least-squares regression
Perhaps other methods focusing on the most important variables found by the regression
tree will give improved results without an extraordinary compute time or storage
requirement. Also, another approach might be to use least-squares on the userId and
movieId without any other knowledge of the dataset. All are compared to merely
predicting the mean of all ratings for every observation.

```{r least-squares-comparison, echo=FALSE}
#least squares predictions
mu_predict <- function(train, test) {
    mu <- mean(train$rating)
    y_hat <- (test$rating*0 + mu)
    return(RMSE(test$rating, y_hat))
}

user_predict <- function(train, test) {
    mu <- mean(train$rating)
    user_effects <- train %>% group_by(userId) %>% summarize(mean = mean(rating-mu))
    user_effects <- left_join(select(test, userId), user_effects, by="userId")
    y_hat <- mu + user_effects$mean
    y_hat[is.na(y_hat) == TRUE] <- mu
    return(RMSE(test$rating, y_hat))
}
user_movie_predict <- function(train, test) {
    mu <- mean(train$rating)
    user_effects <- train %>% group_by(userId) %>%
                              summarize(u_effect = mean(rating-mu))
    movie_effects <- train %>% left_join(user_effects, by="userId") %>%
                               group_by(movieId) %>%
                               summarize(m_effect = mean(rating-mu-u_effect))
    y_hat <- test %>% left_join(user_effects, by="userId") %>% 
                      left_join(movie_effects, by="movieId") %>%
                      mutate(y_hat = mu+u_effect+m_effect) %>%
                      pull(y_hat)
    return(RMSE(test$rating, y_hat))
}
user_movie_effects_penalized_predict <- function(lambda1, lambda2, train, test) {
    library(dplyr)
    mu <- mean(train$rating)
    user_effects <- train %>% group_by(userId) %>%
                              summarize(u_effect = sum(rating-mu)/(n()+lambda1))
    movie_effects <- train %>% left_join(user_effects, by="userId") %>%
                               group_by(movieId) %>%
                               summarize(m_effect = sum(rating-mu-u_effect)/(n()+lambda2))
    y_hat <- test %>% left_join(user_effects, by="userId") %>% 
                      left_join(movie_effects, by="movieId") %>%
                      mutate(y_hat = mu+u_effect+m_effect) %>%
                      pull(y_hat)
    RMSE <- function(y, y_hat){
        sqrt(mean((y - y_hat)^2))
    }
    return(RMSE(y_hat, test$rating))
}

movie_predict <- function(train, test) {
    mu <- mean(train$rating)
    movie_effects <- train %>% group_by(movieId) %>% summarize(mean = mean(rating-mu))
    movie_effects <- left_join(select(test, movieId), movie_effects, by="movieId")
    y_hat <- mu + movie_effects$mean
    y_hat[is.na(y_hat) == TRUE] <- mu
    return(RMSE(test$rating, y_hat))
}
releaseyear_movie_predict <- function(train, test) {
    mu <- mean(train$rating)
    releaseyear_effects <- train %>% group_by(releaseyear) %>%
                              summarize(ry_effect = mean(rating-mu))
    movie_effects <- train %>% left_join(releaseyear_effects, by="releaseyear") %>%
                                     group_by(movieId) %>%
                                     summarize(m_effect = mean(rating-mu-ry_effect))
    y_hat <- test %>% left_join(releaseyear_effects, by="releaseyear") %>%
                      left_join(movie_effects, by="movieId") %>% 
                      mutate(y_hat = mu+m_effect+ry_effect) %>%
                      pull(y_hat)
    return(RMSE(test$rating, y_hat))
}
week_releaseyear_movie_predict <- function(train, test) {
    mu <- mean(train$rating)
    week_effects <- train %>% group_by(week) %>%
                              summarize(w_effect = mean(rating-mu))
    releaseyear_effects <- train %>% left_join(week_effects, by="week") %>%
                                     group_by(releaseyear) %>%
                                     summarize(ry_effect = mean(rating-mu-w_effect))
    movie_effects <- train %>% left_join(week_effects, by="week") %>%
                               left_join(releaseyear_effects, by="releaseyear") %>%
                               group_by(movieId) %>%
                               summarize(m_effect = mean(rating-mu-w_effect-ry_effect))
    y_hat <- test %>% left_join(week_effects, by="week") %>% 
                      left_join(releaseyear_effects, by="releaseyear") %>%
                      left_join(movie_effects, by="movieId") %>%
                      mutate(y_hat = mu+w_effect+ry_effect+m_effect) %>%
                      pull(y_hat)
    return(RMSE(test$rating, y_hat))
}
drama_week_releaseyear_movie_predict <- function(train, test) {
    mu <- mean(train$rating)
    drama_effects <- train %>% group_by(Drama) %>%
                               summarize(d_effect = mean(rating-mu))
    week_effects <- train %>% left_join(drama_effects, by="Drama") %>%
                              group_by(week) %>%
                              summarize(w_effect = mean(rating-mu-d_effect))
    releaseyear_effects <- train %>% left_join(drama_effects, by="Drama") %>%
                               left_join(week_effects, by="week") %>%
                               group_by(releaseyear) %>%
                               summarize(ry_effect = mean(rating-mu-d_effect-w_effect))
    movie_effects <- train %>% left_join(drama_effects, by="Drama") %>%
                              left_join(week_effects, by="week") %>%
                              left_join(releaseyear_effects, by="releaseyear") %>%
                              group_by(movieId) %>%
                              summarize(m_effect = mean(rating-mu-d_effect-w_effect-ry_effect))
    y_hat <- test %>% left_join(drama_effects, by="Drama") %>% 
                      left_join(week_effects, by="week") %>%
                      left_join(releaseyear_effects, by="releaseyear") %>%
                      left_join(movie_effects, by="movieId") %>%
                      mutate(y_hat = mu+d_effect+w_effect+ry_effect+m_effect) %>%
                      pull(y_hat)
    return(RMSE(test$rating, y_hat))
}


results <- data.frame(method = "Mean", RMSE = mu_predict(train, test))
results <- rbind(results, data.frame(method = "Users", RMSE = user_predict(train, test)))
results <- rbind(results, data.frame(method = "Users+Movies", RMSE = user_movie_predict(train, test)))
#results <- rbind(results, data.frame(method = "Users+Movies (penalized)", RMSE = user_movie_effects_penalized_predict(10,10,train, test)))
results <- rbind(results, data.frame(method = "Movies", RMSE = movie_predict(train, test)))
results <- rbind(results, data.frame(method = "Movies+ReleaseYear", RMSE = releaseyear_movie_predict(train, test)))
results <- rbind(results, data.frame(method = "Movies+ReleaseYear+Drama", RMSE = week_releaseyear_movie_predict(train, test)))
results <- rbind(results, data.frame(method = "Movies+ReleaseYear+Drama+Week", RMSE = drama_week_releaseyear_movie_predict(train, test)))
print(results)
```

Note that MovieId + UserId is much better than any of the MovieId plus other options.

## 2.3 Penalized Least Squares and multiple lambdas

Penalized (or normalized) least squares reduces the influence of predictor means with 
few samples. Standard penalized least squares has one penalty parameter (lambda); there
is no reason not to use one lambda per predictor (movieId and userId). These lambdas
can be tuned to the training set and the results graphed as a contour plot:

```{r penalized-tune, echo=FALSE}
user_movie_effects_penalized <- function(train, lambda1, lambda2) {
    mu <- mean(train$rating)
    user_effects <- train %>% group_by(userId) %>%
                              summarize(u_effect = sum(rating-mu)/(n()+lambda1))
    movie_effects <- train %>% left_join(user_effects, by="userId") %>%
                               group_by(movieId) %>%
                               summarize(m_effect = sum(rating-mu-u_effect)/(n()+lambda2))
    y_hat <- train %>% left_join(user_effects, by="userId") %>% 
                      left_join(movie_effects, by="movieId") %>%
                      mutate(y_hat = mu+u_effect+m_effect) %>%
                      pull(y_hat)
    return(y_hat)
}

user_movie_effects_penalized_predict <- function(lambda1, lambda2, train, test) {
    library(dplyr)
    mu <- mean(train$rating)
    user_effects <- train %>% group_by(userId) %>%
                              summarize(u_effect = sum(rating-mu)/(n()+lambda1))
    movie_effects <- train %>% left_join(user_effects, by="userId") %>%
                               group_by(movieId) %>%
                               summarize(m_effect = sum(rating-mu-u_effect)/(n()+lambda2))
    y_hat <- test %>% left_join(user_effects, by="userId") %>% 
                      left_join(movie_effects, by="movieId") %>%
                      mutate(y_hat = mu+u_effect+m_effect) %>%
                      pull(y_hat)
    RMSE <- function(y, y_hat){
        sqrt(mean((y - y_hat)^2))
    }
    return(RMSE(y_hat, test$rating))
}

#There can be a lambda for each factor vs. one lambda for all factors. These are 
#not overly difficult to tune independently.
library(parallel)
cl <- makeCluster(detectCores()/2)
penalized_tune <- parSapplyLB(cl,seq(0,20,.5), user_movie_effects_penalized_predict, 0.0, train, test)
penalized_tune <- cbind(penalized_tune, parSapplyLB(cl,seq(0,20,.5), user_movie_effects_penalized_predict, 0.5, train, test))
penalized_tune <- cbind(penalized_tune, parSapplyLB(cl,seq(0,20,.5), user_movie_effects_penalized_predict, 1.0, train, test))
penalized_tune <- cbind(penalized_tune, parSapplyLB(cl,seq(0,20,.5), user_movie_effects_penalized_predict, 1.5, train, test))
penalized_tune <- cbind(penalized_tune, parSapplyLB(cl,seq(0,20,.5), user_movie_effects_penalized_predict, 2.0, train, test))
penalized_tune <- cbind(penalized_tune, parSapplyLB(cl,seq(0,20,.5), user_movie_effects_penalized_predict, 2.5, train, test))
penalized_tune <- cbind(penalized_tune, parSapplyLB(cl,seq(0,20,.5), user_movie_effects_penalized_predict, 3.0, train, test))
penalized_tune <- cbind(penalized_tune, parSapplyLB(cl,seq(0,20,.5), user_movie_effects_penalized_predict, 3.5, train, test))
penalized_tune <- cbind(penalized_tune, parSapplyLB(cl,seq(0,20,.5), user_movie_effects_penalized_predict, 4.0, train, test))
penalized_tune <- cbind(penalized_tune, parSapplyLB(cl,seq(0,20,.5), user_movie_effects_penalized_predict, 4.5, train, test))
penalized_tune <- cbind(penalized_tune, parSapplyLB(cl,seq(0,20,.5), user_movie_effects_penalized_predict, 5.0, train, test))
penalized_tune <- cbind(penalized_tune, parSapplyLB(cl,seq(0,20,.5), user_movie_effects_penalized_predict, 5.5, train, test))
penalized_tune <- cbind(penalized_tune, parSapplyLB(cl,seq(0,20,.5), user_movie_effects_penalized_predict, 6.0, train, test))
penalized_tune <- cbind(penalized_tune, parSapplyLB(cl,seq(0,20,.5), user_movie_effects_penalized_predict, 6.5, train, test))
penalized_tune <- cbind(penalized_tune, parSapplyLB(cl,seq(0,20,.5), user_movie_effects_penalized_predict, 7.0, train, test))
penalized_tune <- cbind(penalized_tune, parSapplyLB(cl,seq(0,20,.5), user_movie_effects_penalized_predict, 7.5, train, test))
penalized_tune <- cbind(penalized_tune, parSapplyLB(cl,seq(0,20,.5), user_movie_effects_penalized_predict, 8.0, train, test))
penalized_tune <- cbind(penalized_tune, parSapplyLB(cl,seq(0,20,.5), user_movie_effects_penalized_predict, 8.5, train, test))
penalized_tune <- cbind(penalized_tune, parSapplyLB(cl,seq(0,20,.5), user_movie_effects_penalized_predict, 9.0, train, test))
penalized_tune <- cbind(penalized_tune, parSapplyLB(cl,seq(0,20,.5), user_movie_effects_penalized_predict, 9.5, train, test))
penalized_tune <- cbind(penalized_tune, parSapplyLB(cl,seq(0,20,.5), user_movie_effects_penalized_predict, 10.0, train, test))
stopCluster(cl)
#print(penalized_tune)
filled.contour(seq(0,20,.5),seq(0,10,.5),penalized_tune,xlab="lambda1",ylab="lambda2")
```

The optimal lambda1 is 17.5 and lambda2 is 2.5. The RMSE with those parameters is: 

```{r penalized-predict, echo=FALSE}
lambda1 <- 17.5
lambda2 <- 2.5
pm_RMSE <- user_movie_effects_penalized_predict(lambda1, lambda2, train, test)
cat("penalized model RMSE = ", pm_RMSE, "\n")
```

## 2.3 Hybrid approaches

After using the penalized least-squared prediction algorithm, there is still a residual left over. This can be subtracted out and the residuals predicted or analyzed using another algorithm. First the regression tree is tried.

```{r hybrid-model-1-analyze, echo=FALSE}
mu <- mean(train$rating)
user_effects <- train %>% group_by(userId) %>%
                          summarize(u_effect = sum(rating-mu)/(n()+lambda1))
movie_effects <- train %>% left_join(user_effects, by="userId") %>%
                          group_by(movieId) %>%
                          summarize(m_effect = sum(rating-mu-u_effect)/(n()+lambda2))
y_hat <- train %>% left_join(user_effects, by="userId") %>% 
                  left_join(movie_effects, by="movieId") %>%
                  mutate(y_hat = mu+u_effect+m_effect) %>%
                  pull(y_hat)
train2 <- train %>% mutate(residual=rating-y_hat) %>% select(-userId,-movieId,-rating)
fit <- rpart(residual ~ ., data=train2, control=rpart.control(cp = .0002, maxsurrogate=0))
plot(fit, margin = 0.1)
text(fit, cex = 0.75)
fit <- rpart(residual ~ ., data=train2, control=rpart.control(cp = 0,minsplit=500, maxsurrogate=0))
barplot(fit$variable.importance, las=2)
```

Then the RMSE for the train and test set using the original tuning is:

```{r hybrid-model-1-predict, echo=FALSE}
y_hat <- test %>% left_join(user_effects, by="userId") %>% 
                  left_join(movie_effects, by="movieId") %>%
                  mutate(y_hat = mu+u_effect+m_effect) %>%
                  pull(y_hat)
test2 <- test %>% mutate(residual=rating-y_hat) %>% select(-userId,-movieId,-rating)
hm1_RMSE <- RMSE(test$rating, y_hat+predict(fit, newdata=test2))
cat("hybrid model 1 RMSE = ", hm1_RMSE, "\n")
```
```{r gc1, echo=FALSE, results="hide"}
rm(user_effects,movie_effects,y_hat,train2,test2,fit)
gc()
```

The tuning for the minsplit parameter may be different for the residual vs. the
original ratings, though.

```{r hybrid-model-1-retune, echo=FALSE}
hybrid_model_1_predict <- function(minsplit, train, test) {
    g <- gc()
    library(dplyr)
    library(rpart)
    lambda1 <- 17.5
    lambda2 <- 2.5
    mu <- mean(train$rating)
    user_effects <- train %>% group_by(userId) %>%
                              summarize(u_effect = sum(rating-mu)/(n()+lambda1))
    movie_effects <- train %>% left_join(user_effects, by="userId") %>%
                               group_by(movieId) %>%
                               summarize(m_effect = sum(rating-mu-u_effect)/(n()+lambda2))
    y_hat <- train %>% left_join(user_effects, by="userId") %>% 
                      left_join(movie_effects, by="movieId") %>%
                      mutate(y_hat = mu+u_effect+m_effect) %>%
                      pull(y_hat)
    train <- train %>% mutate(residual = rating - y_hat)
    fit <- rpart(residual ~ .-rating-userId-movieId, data=train, control=rpart.control(cp = 0,minsplit=minsplit, maxsurrogate=0))
    y_hat <- test %>% left_join(user_effects, by="userId") %>% 
                      left_join(movie_effects, by="movieId") %>%
                      mutate(y_hat = mu+u_effect+m_effect) %>%
                      pull(y_hat)
    y_hat <- y_hat + predict(fit, newdata=test)
    RMSE <- function(y, y_hat){
        sqrt(mean((y - y_hat)^2))
    }
    return(RMSE(test$rating, y_hat))
}
cl <- makeCluster(2)
tune3 <- c(100000,50000,20000,10000,5000,2000,1000,500,200)
hybrid_1_tune <- parSapplyLB(cl, tune3, hybrid_model_1_predict, train, test)
plot(tune3, hybrid_1_tune, type="b")
stopCluster(cl)
rmse_1 <- hybrid_model_1_predict(2000,train, test)
cat("Model with penalized effects plus tree, RMSE=", rmse_1, "\n")
hybrid_model_1 <- function(minsplit, train, test) {
    g <- gc()
    library(dplyr)
    library(rpart)
    lambda1 <- 17.5
    lambda2 <- 2.5
    mu <- mean(train$rating)
    user_effects <- train %>% group_by(userId) %>%
                              summarize(u_effect = sum(rating-mu)/(n()+lambda1))
    movie_effects <- train %>% left_join(user_effects, by="userId") %>%
                               group_by(movieId) %>%
                               summarize(m_effect = sum(rating-mu-u_effect)/(n()+lambda2))
    y_hat <- train %>% left_join(user_effects, by="userId") %>% 
                      left_join(movie_effects, by="movieId") %>%
                      mutate(y_hat = mu+u_effect+m_effect) %>%
                      pull(y_hat)
    train <- train %>% mutate(residual = rating - y_hat)
    fit <- rpart(residual ~ .-rating-userId-movieId, data=train, control=rpart.control(cp = 0,minsplit=minsplit, maxsurrogate=0))
    return(fit)
}
fit <- hybrid_model_1(2000,train,test)
barplot(fit$variable.importance, las=2)
```

```{r gc,echo=FALSE,results="hide"}
rm(fit)
gc()
```

## 2.4 Generalized Additive Model (GAM)

An alternative to the regression tree is a Generalized Additive Model. A GAM is an
extension of a GLM (Generalized Linear Model) except smooth functions of 
predictors such as cubic splines are fitted to the data rather than linear functions,
giving more flexibility. R has an efficient algorithm to train and predict GAM models
so it can be used on the most important predictors from the regression tree results.

```{r gam-predict, echo=FALSE}
# Try a generalized additive model on the 3 most important variables from the residual
hybrid_model_2 <- function(train, test) {
    g <- gc()
    library(mgcv)
    library(dplyr)
    library(parallel)
    library(rpart)
    lambda1 <- 17.5
    lambda2 <- 2.5
    mu <- mean(train$rating)
    user_effects <- train %>% group_by(userId) %>%
                              summarize(u_effect = sum(rating-mu)/(n()+lambda1))
    movie_effects <- train %>% left_join(user_effects, by="userId") %>%
                               group_by(movieId) %>%
                               summarize(m_effect = sum(rating-mu-u_effect)/(n()+lambda2))
    y_hat <- train %>% left_join(user_effects, by="userId") %>% 
                      left_join(movie_effects, by="movieId") %>%
                      mutate(y_hat = mu+u_effect+m_effect) %>%
                      pull(y_hat)
    train <- train %>% mutate(residual = rating - y_hat)
    fit <- bam(residual ~ s(as.numeric(week),bs="cs")+s(releaseyear,bs="cs")+s(day,bs="cs"),
               data=train, gc.level=2)
    return(fit)
}

fit <- hybrid_model_2(train,test)
print(summary(fit))

hybrid_model_2_predict <- function(train, test) {
    g <- gc()
    library(mgcv)
    library(dplyr)
    library(parallel)
    library(rpart)
    lambda1 <- 17.5
    lambda2 <- 2.5
    cl <- makeCluster(detectCores()/4)
    mu <- mean(train$rating)
    user_effects <- train %>% group_by(userId) %>%
                              summarize(u_effect = sum(rating-mu)/(n()+lambda1))
    movie_effects <- train %>% left_join(user_effects, by="userId") %>%
                               group_by(movieId) %>%
                               summarize(m_effect = sum(rating-mu-u_effect)/(n()+lambda2))
    y_hat <- train %>% left_join(user_effects, by="userId") %>% 
                      left_join(movie_effects, by="movieId") %>%
                      mutate(y_hat = mu+u_effect+m_effect) %>%
                      pull(y_hat)
    train <- train %>% mutate(residual = rating - y_hat)
    fit <- bam(residual ~ s(as.numeric(week),bs="cs")+s(releaseyear,bs="cs")+s(day,bs="cs"),
               data=train, gc.level=2, cluster=cl)
    y_hat <- test %>% left_join(user_effects, by="userId") %>% 
                      left_join(movie_effects, by="movieId") %>%
                      mutate(y_hat = mu+u_effect+m_effect) %>%
                      pull(y_hat)
    y_hat <- y_hat + predict(fit, newdata=test)
    RMSE <- function(y, y_hat){
        sqrt(mean((y - y_hat)^2))
    }
    stopCluster(cl)
    return(RMSE(test$rating, y_hat))
}
rmse_2 <- hybrid_model_2_predict(train, test)
cat("Model with penalized effects plus GAM, RMSE=", rmse_2, "\n")
rm(train,test)
```

The GAM results for 3 individual predictors are not quite as good as the results for
the regression tree with all remaining predictors (minus movieId and userId).

# 3 Results

The best results so far are from penalized regression on userId and movieId and a
regression tree for the residual from the rest of the predictors. Training this model
on the full edx set to predict the validation set gives a result:

```{r final-wrangle, echo=FALSE, results="hide"}
#Use model 1 (penalized least squares plus regression tree) to predict validation set
gc()
edx <- wrangle(edx)
validation <- wrangle(validation)
```
```{r final_predict, echo=FALSE}
final_RMSE <- hybrid_model_1_predict(2000,edx, validation)
cat("Final RMSE for full edx/validation sets = ", final_RMSE, "\n")
```

This is not as good as the .859 target.

# 4 Conclusion

A hybrid approach with 2 levels of prediction was able to achieve a better result
than either algorithm alone, but not to the level required by the parameters of
the project. However, other approaches exist that were not implemented due to time
and resource constraints. Based on the results, one called gradient boosting trees
that iterates building regression trees on the residuals of the previous tree seems
promising, though overtraining seems likely.

GAM has a large number of options for building a model; only a model of form
f(x)+f(y)+f(z) was used when models such as f(x)+f(y)+f(z)+g(x,y,z), etc. are 
permissible. Exploring them may give the model additional predictive power.